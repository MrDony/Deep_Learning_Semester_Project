{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "_dataset_directory = \"drive/MyDrive/DL_PROJECT_DATASET_V2\"\n",
    "_test_directory = \"drive/MyDrive/DL_PROJECT_TEST_DATASET_V2\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def preprocess_image(img, n_colors=5):\n",
    "    # Convert from float32 to uint8 and from RGB to BGR\n",
    "    image = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Convert to grayscale for edge detection\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Edge detection\n",
    "    edges = cv2.Canny(gray_image, 100, 200)\n",
    "    resized_edges = cv2.resize(edges, (224, 224))\n",
    "\n",
    "    # Top n colors\n",
    "    pixels = image.reshape(-1, 3)\n",
    "    kmeans = KMeans(n_clusters=n_colors, n_init=10, random_state=0).fit(pixels)\n",
    "    prominent_colors = kmeans.cluster_centers_.astype(int)\n",
    "\n",
    "    resized_image = cv2.resize(image, (224, 224))\n",
    "    return resized_image, resized_edges, prominent_colors\n",
    "\n",
    "def custom_generator(image_data_generator, steps_per_epoch):\n",
    "    batch_count = 0\n",
    "    while True:\n",
    "        for batch_x, batch_y in image_data_generator:\n",
    "            batch_x_original = np.zeros((batch_x.shape[0], 224, 224, 3))\n",
    "            batch_x_edges = np.zeros((batch_x.shape[0], 224, 224, 1))\n",
    "            batch_x_colors = np.zeros((batch_x.shape[0], 5, 3))\n",
    "\n",
    "            for i, img in enumerate(batch_x):\n",
    "                original, edges, colors = preprocess_image(img)\n",
    "                batch_x_original[i] = cv2.cvtColor(original, cv2.COLOR_BGR2RGB) / 255.0\n",
    "                batch_x_edges[i] = np.expand_dims(edges, axis=-1) / 255.0\n",
    "                batch_x_colors[i] = colors / 255.0\n",
    "\n",
    "            yield [batch_x_original, batch_x_edges, batch_x_colors], batch_y\n",
    "\n",
    "            batch_count += 1\n",
    "            if batch_count >= steps_per_epoch:\n",
    "                batch_count = 0\n",
    "                break\n",
    "\n",
    "# Define CNN structure\n",
    "def create_cnn(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, (2, 2), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(8, (2, 2), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "def create_cnn_for_colors(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (1, 1), activation='relu')(input_layer)  # Using smaller kernel size\n",
    "    x = Flatten()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# Create inputs for each image type\n",
    "input_original = Input(shape=(224, 224, 3))\n",
    "input_edges = Input(shape=(224, 224, 1))\n",
    "input_colors = Input(shape=(5, 3))  # Assuming 5 prominent colors\n",
    "\n",
    "# Create CNNs\n",
    "cnn_original = create_cnn((224, 224, 3))\n",
    "cnn_edges = create_cnn((224, 224, 1))\n",
    "cnn_colors = create_cnn_for_colors((5, 3, 1))  # Note: This architecture might need adjustment\n",
    "\n",
    "# Get outputs from CNNs\n",
    "output_original = cnn_original(input_original)\n",
    "output_edges = cnn_edges(input_edges)\n",
    "output_colors = cnn_colors(Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_colors))\n",
    "\n",
    "# Concatenate outputs\n",
    "concatenated = Concatenate()([output_original, output_edges, output_colors])\n",
    "\n",
    "# Dense layers for classification\n",
    "dense = Dense(64, activation='relu')(concatenated)\n",
    "dense = Dense(16, activation='relu')(dense)\n",
    "output_layer = Dense(5, activation='softmax')(dense)  # Assuming 6 classes\n",
    "\n",
    "# Complete model\n",
    "model = Model(inputs=[input_original, input_edges, input_colors], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "total_train_samples = 343\n",
    "total_val_samples = 83\n",
    "total_test_samples = 54\n",
    "batch_size = 32\n",
    "\n",
    "train_steps = total_train_samples // batch_size + (1 if total_train_samples % batch_size else 0)\n",
    "val_steps = total_val_samples // batch_size + (1 if total_val_samples % batch_size else 0)\n",
    "test_steps = total_test_samples // batch_size + (1 if total_test_samples % batch_size else 0)\n",
    "\n",
    "print('train')\n",
    "train_data_gen = train_datagen.flow_from_directory(\n",
    "    _dataset_directory,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "class_names = list(train_data_gen.class_indices.keys())\n",
    "print(class_names)\n",
    "# Get the class indices mapping\n",
    "class_indices = train_data_gen.class_indices\n",
    "\n",
    "# Create an empty dictionary to store the count for each class\n",
    "class_count = {class_name: 0 for class_name in class_indices.keys()}\n",
    "\n",
    "# Crawl through the directory\n",
    "for root, _, files in os.walk(_dataset_directory):\n",
    "    for file in files:\n",
    "        # Get the class name from the file path\n",
    "        class_name = os.path.basename(root)\n",
    "        # Increment the count for the respective class\n",
    "        class_count[class_name] += 1\n",
    "\n",
    "# Print the number of elements in each class\n",
    "for class_name, count in class_count.items():\n",
    "    print(f\"Class '{class_name}': {count} elements\")\n",
    "\n",
    "\n",
    "print('validation')\n",
    "val_data_gen = validation_datagen.flow_from_directory(\n",
    "    _dataset_directory,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "class_names = list(val_data_gen.class_indices.keys())\n",
    "print(class_names)\n",
    "\n",
    "print('test')\n",
    "test_data_gen = test_datagen.flow_from_directory(\n",
    "    _test_directory,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "class_names = list(test_data_gen.class_indices.keys())\n",
    "print(class_names)\n",
    "\n",
    "train_generator = custom_generator(train_data_gen, train_steps)\n",
    "validation_generator = custom_generator(val_data_gen, val_steps)\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "version = \"5.2\"\n",
    "checkpoint = ModelCheckpoint(\"./model_V\"+version+\".h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# ModelCheckpoint to save the model after every epoch\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'art_style_model_best_v'+version+'.h5',\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# EarlyStopping to stop training when the validation loss has not improved after 5 epochs\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau to reduce the learning rate when the validation loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('art_style_model_v'+version+'.h5')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'loss': [1.6337611675262451, 1.5539116859436035, 1.4851839542388916, 1.3919838666915894, 1.4624838829040527, 1.3689720630645752, 1.3602548837661743, 1.2213523387908936, 1.1766377687454224, 1.082789659500122], 'accuracy': [0.24147726595401764, 0.3323170840740204, 0.3323170840740204, 0.3841463327407837, 0.2926829159259796, 0.37804877758026123, 0.3628048896789551, 0.5182926654815674, 0.46341463923454285, 0.5304877758026123], 'val_loss': [1.5759292840957642, 1.5368410348892212, 1.3589630126953125, 1.3686589002609253, 1.4192320108413696, 1.3711737394332886, 1.2673320770263672, 1.1651921272277832, 1.19804048538208, 1.03669011592865], 'val_accuracy': [0.35555556416511536, 0.2888889014720917, 0.42222222685813904, 0.35555556416511536, 0.4000000059604645, 0.3777777850627899, 0.41111111640930176, 0.4888888895511627, 0.3888888955116272, 0.6000000238418579], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
