{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi CNN + Edge + Segmentation\n",
    "# Monitored for validation loss\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "_dataset_directory = \"drive/MyDrive/DL_PROJECT_DATASET_V1\"\n",
    "\n",
    "# Define CNN structure\n",
    "def create_cnn(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(16, (2, 2), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "def create_cnn_for_colors(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (1, 1), activation='relu')(input_layer)  # Using smaller kernel size\n",
    "    x = Flatten()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# Create inputs for each image type\n",
    "input_original = Input(shape=(224, 224, 3))\n",
    "input_edges = Input(shape=(224, 224, 1))\n",
    "input_colors = Input(shape=(5, 3))  # Assuming 5 prominent colors\n",
    "\n",
    "# Create CNNs\n",
    "cnn_original = create_cnn((224, 224, 3))\n",
    "cnn_edges = create_cnn((224, 224, 1))\n",
    "cnn_colors = create_cnn_for_colors((5, 3, 1))  # Note: This architecture might need adjustment\n",
    "\n",
    "# Get outputs from CNNs\n",
    "output_original = cnn_original(input_original)\n",
    "output_edges = cnn_edges(input_edges)\n",
    "output_colors = cnn_colors(Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_colors))\n",
    "\n",
    "# Concatenate outputs\n",
    "concatenated = Concatenate()([output_original, output_edges, output_colors])\n",
    "\n",
    "# Dense layers for classification\n",
    "dense = Dense(128, activation='relu')(concatenated)\n",
    "output_layer = Dense(6, activation='softmax')(dense)  # Assuming 6 classes\n",
    "\n",
    "# Complete model\n",
    "model = Model(inputs=[input_original, input_edges, input_colors], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def preprocess_image(img, n_colors=5):\n",
    "    # Convert from float32 to uint8 and from RGB to BGR\n",
    "    image = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Convert to grayscale for edge detection\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Edge detection\n",
    "    edges = cv2.Canny(gray_image, 100, 200)\n",
    "    resized_edges = cv2.resize(edges, (224, 224))\n",
    "\n",
    "    # Top n colors\n",
    "    pixels = image.reshape(-1, 3)\n",
    "    kmeans = KMeans(n_clusters=n_colors, n_init=10, random_state=0).fit(pixels)\n",
    "    prominent_colors = kmeans.cluster_centers_.astype(int)\n",
    "\n",
    "    resized_image = cv2.resize(image, (224, 224))\n",
    "    return resized_image, resized_edges, prominent_colors\n",
    "\n",
    "def custom_generator(image_data_generator, steps_per_epoch):\n",
    "    batch_count = 0\n",
    "    while True:\n",
    "        for batch_x, batch_y in image_data_generator:\n",
    "            batch_x_original = np.zeros((batch_x.shape[0], 224, 224, 3))\n",
    "            batch_x_edges = np.zeros((batch_x.shape[0], 224, 224, 1))\n",
    "            batch_x_colors = np.zeros((batch_x.shape[0], 5, 3))\n",
    "\n",
    "            for i, img in enumerate(batch_x):\n",
    "                original, edges, colors = preprocess_image(img)\n",
    "                batch_x_original[i] = cv2.cvtColor(original, cv2.COLOR_BGR2RGB) / 255.0\n",
    "                batch_x_edges[i] = np.expand_dims(edges, axis=-1) / 255.0\n",
    "                batch_x_colors[i] = colors / 255.0\n",
    "\n",
    "            yield [batch_x_original, batch_x_edges, batch_x_colors], batch_y\n",
    "\n",
    "            batch_count += 1\n",
    "            if batch_count >= steps_per_epoch:\n",
    "                batch_count = 0\n",
    "                break\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2, \n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "\n",
    "total_train_samples = 343\n",
    "total_val_samples = 83\n",
    "batch_size = 32\n",
    "\n",
    "train_steps = total_train_samples // batch_size + (1 if total_train_samples % batch_size else 0)\n",
    "val_steps = total_val_samples // batch_size + (1 if total_val_samples % batch_size else 0)\n",
    "\n",
    "train_data_gen = train_datagen.flow_from_directory(\n",
    "    _dataset_directory, \n",
    "    target_size=(224, 224), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='categorical', \n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_data_gen = validation_datagen.flow_from_directory(\n",
    "    _dataset_directory, \n",
    "    target_size=(224, 224), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='categorical', \n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "train_generator = custom_generator(train_data_gen, train_steps)\n",
    "validation_generator = custom_generator(val_data_gen, val_steps)\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./model_V3.1.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# ModelCheckpoint to save the model after every epoch\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'art_style_model_best_v3.1.h5', \n",
    "    monitor='val_loss', \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# EarlyStopping to stop training when the validation loss has not improved after 5 epochs\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5, \n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau to reduce the learning rate when the validation loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.2, \n",
    "    patience=3, \n",
    "    verbose=1, \n",
    "    mode='min',\n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('art_style_model_v3.1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "history = {'loss': [4.457160472869873, 1.716261625289917, 1.4640203714370728, 1.260480284690857, 1.1083595752716064, 1.063092589378357, 1.0915950536727905, 0.9701434969902039, 0.9114781022071838, 0.871853768825531, 0.9130118489265442, 0.8544056415557861, 0.8068971037864685, 0.7558664083480835, 0.751266598701477, 0.7298506498336792, 0.7162065505981445, 0.7210341691970825, 0.7144117951393127, 0.7021090984344482, 0.6917153596878052, 0.6845459938049316, 0.6965336203575134, 0.6972512602806091], 'accuracy': [0.22448979318141937, 0.33236151933670044, 0.4810495674610138, 0.5160349607467651, 0.5743440389633179, 0.5830903649330139, 0.588921308517456, 0.6355684995651245, 0.6763848662376404, 0.6501457691192627, 0.6647230386734009, 0.6705539226531982, 0.7026239037513733, 0.7201166152954102, 0.7142857313156128, 0.7259474992752075, 0.7259474992752075, 0.7317784428596497, 0.7346938848495483, 0.7405247688293457, 0.7492711544036865, 0.7434402108192444, 0.7463557124137878, 0.7492711544036865], 'val_loss': [2.0124619007110596, 1.5428327322006226, 1.4092859029769897, 1.2252542972564697, 1.053996205329895, 0.9147937893867493, 0.9492437243461609, 0.9353174567222595, 0.8333977460861206, 0.8799031376838684, 0.8576726317405701, 0.8942350745201111, 0.8115752935409546, 0.6981508731842041, 0.7377256155014038, 0.7041245698928833, 0.7449336051940918, 0.6957244873046875, 0.6725016832351685, 0.701926052570343, 0.7784252762794495, 0.7231735587120056, 0.7501319050788879, 0.8031225800514221], 'val_accuracy': [0.20481927692890167, 0.5662650465965271, 0.4337349534034729, 0.5180723071098328, 0.6144578456878662, 0.6626505851745605, 0.6024096608161926, 0.6987951993942261, 0.6867470145225525, 0.6746987700462341, 0.5903614163398743, 0.6385542154312134, 0.6746987700462341, 0.7349397540092468, 0.7108433842658997, 0.759036123752594, 0.6867470145225525, 0.7469879388809204, 0.759036123752594, 0.7349397540092468, 0.7349397540092468, 0.7228915691375732, 0.6746987700462341, 0.6867470145225525], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 0.00020000001, 4.0000003e-05, 4.0000003e-05, 4.0000003e-05, 4.0000003e-05, 4.0000003e-05, 1e-05, 1e-05]}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
