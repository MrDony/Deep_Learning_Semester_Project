{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "_dataset_directory = \"drive/MyDrive/DL_PROJECT_DATASET_V1\"\n",
    "_test_directory = \"drive/MyDrive/DL_PROJECT_TEST_DATASET_V1\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def preprocess_image(img, n_colors=5):\n",
    "    # Convert from float32 to uint8 and from RGB to BGR\n",
    "    image = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Convert to grayscale for edge detection\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Edge detection\n",
    "    edges = cv2.Canny(gray_image, 100, 200)\n",
    "    resized_edges = cv2.resize(edges, (224, 224))\n",
    "\n",
    "    # Top n colors\n",
    "    pixels = image.reshape(-1, 3)\n",
    "    kmeans = KMeans(n_clusters=n_colors, n_init=10, random_state=0).fit(pixels)\n",
    "    prominent_colors = kmeans.cluster_centers_.astype(int)\n",
    "\n",
    "    resized_image = cv2.resize(image, (224, 224))\n",
    "    return resized_image, resized_edges, prominent_colors\n",
    "\n",
    "def custom_generator(image_data_generator, steps_per_epoch):\n",
    "    batch_count = 0\n",
    "    while True:\n",
    "        for batch_x, batch_y in image_data_generator:\n",
    "            batch_x_original = np.zeros((batch_x.shape[0], 224, 224, 3))\n",
    "            batch_x_edges = np.zeros((batch_x.shape[0], 224, 224, 1))\n",
    "            batch_x_colors = np.zeros((batch_x.shape[0], 5, 3))\n",
    "\n",
    "            for i, img in enumerate(batch_x):\n",
    "                original, edges, colors = preprocess_image(img)\n",
    "                batch_x_original[i] = cv2.cvtColor(original, cv2.COLOR_BGR2RGB) / 255.0\n",
    "                batch_x_edges[i] = np.expand_dims(edges, axis=-1) / 255.0\n",
    "                batch_x_colors[i] = colors / 255.0\n",
    "\n",
    "            yield [batch_x_original, batch_x_edges, batch_x_colors], batch_y\n",
    "\n",
    "            batch_count += 1\n",
    "            if batch_count >= steps_per_epoch:\n",
    "                batch_count = 0\n",
    "                break\n",
    "\n",
    "# Define CNN structure\n",
    "def create_cnn(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(16, (2, 2), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "def create_cnn_for_colors(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (1, 1), activation='relu')(input_layer)  # Using smaller kernel size\n",
    "    x = Flatten()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# Create inputs for each image type\n",
    "input_original = Input(shape=(224, 224, 3))\n",
    "input_edges = Input(shape=(224, 224, 1))\n",
    "input_colors = Input(shape=(5, 3))  # Assuming 5 prominent colors\n",
    "\n",
    "# Create CNNs\n",
    "cnn_original = create_cnn((224, 224, 3))\n",
    "cnn_edges = create_cnn((224, 224, 1))\n",
    "cnn_colors = create_cnn_for_colors((5, 3, 1))  # Note: This architecture might need adjustment\n",
    "\n",
    "# Get outputs from CNNs\n",
    "output_original = cnn_original(input_original)\n",
    "output_edges = cnn_edges(input_edges)\n",
    "output_colors = cnn_colors(Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_colors))\n",
    "\n",
    "# Concatenate outputs\n",
    "concatenated = Concatenate()([output_original, output_edges, output_colors])\n",
    "\n",
    "# Dense layers for classification\n",
    "dense = Dense(128, activation='relu')(concatenated)\n",
    "output_layer = Dense(6, activation='softmax')(dense)  # Assuming 6 classes\n",
    "\n",
    "# Complete model\n",
    "model = Model(inputs=[input_original, input_edges, input_colors], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "total_train_samples = 343\n",
    "total_val_samples = 83\n",
    "total_test_samples = 54\n",
    "batch_size = 32\n",
    "\n",
    "train_steps = total_train_samples // batch_size + (1 if total_train_samples % batch_size else 0)\n",
    "val_steps = total_val_samples // batch_size + (1 if total_val_samples % batch_size else 0)\n",
    "test_steps = total_test_samples // batch_size + (1 if total_test_samples % batch_size else 0)\n",
    "\n",
    "train_data_gen = train_datagen.flow_from_directory(\n",
    "    _dataset_directory,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_data_gen = validation_datagen.flow_from_directory(\n",
    "    _dataset_directory,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_data_gen = test_datagen.flow_from_directory(\n",
    "    _test_directory,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "train_generator = custom_generator(train_data_gen, train_steps)\n",
    "validation_generator = custom_generator(val_data_gen, val_steps)\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./model_V3.3.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# ModelCheckpoint to save the model after every epoch\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'art_style_model_best_v3.3.h5',\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# EarlyStopping to stop training when the validation loss has not improved after 5 epochs\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau to reduce the learning rate when the validation loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('art_style_model_v3.3.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'loss': [1.3677223920822144, 1.2917981147766113, 1.0909900665283203, 1.0387272834777832, 0.9726772308349609, 0.7953414916992188, 0.8516399264335632, 0.8836516737937927, 0.7710679769515991, 0.8679074048995972, 0.7263955473899841, 0.7562497854232788, 0.7076928615570068, 0.6437550783157349, 0.7378449440002441, 0.7285891771316528, 0.6088800430297852, 0.5626384615898132, 0.587087094783783, 0.5227916240692139, 0.5964056849479675, 0.5794382095336914, 0.5608565807342529, 0.5573264956474304, 0.5960157513618469], 'accuracy': [0.4693877696990967, 0.5102040767669678, 0.6034985184669495, 0.5830903649330139, 0.6297376155853271, 0.6851311922073364, 0.647230327129364, 0.6501457691192627, 0.6851311922073364, 0.6705539226531982, 0.7026239037513733, 0.7113702893257141, 0.7230320572853088, 0.7551020383834839, 0.7026239037513733, 0.705539345741272, 0.7842565774917603, 0.7871720194816589, 0.795918345451355, 0.819242000579834, 0.7609329223632812, 0.7900874614715576, 0.8017492890357971, 0.7900874614715576, 0.7813411355018616], 'val_loss': [1.315810203552246, 1.1910982131958008, 1.1532803773880005, 0.9834330677986145, 0.8514275550842285, 0.9633168578147888, 0.8781489133834839, 0.9170699715614319, 0.8699895739555359, 0.8146267533302307, 0.6767652630805969, 0.684617280960083, 0.6725450158119202, 0.6372936367988586, 0.7225661873817444, 0.700664222240448, 0.698101282119751, 0.7462931871414185, 0.6361097693443298, 0.5596312284469604, 0.8618825674057007, 0.6015252470970154, 0.6037704944610596, 0.80154949426651, 0.5815305709838867], 'val_accuracy': [0.4939759075641632, 0.6024096608161926, 0.5783132314682007, 0.5783132314682007, 0.6265060305595398, 0.6265060305595398, 0.6385542154312134, 0.650602400302887, 0.6265060305595398, 0.6385542154312134, 0.7228915691375732, 0.7469879388809204, 0.7469879388809204, 0.7710843086242676, 0.6746987700462341, 0.7108433842658997, 0.7831325531005859, 0.6987951993942261, 0.7469879388809204, 0.8072289228439331, 0.7108433842658997, 0.7710843086242676, 0.7951807379722595, 0.6746987700462341, 0.7469879388809204], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
